{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from string import ascii_lowercase\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the file previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/users/tyler/desktop/texts_lammetized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>handle_id</th>\n",
       "      <th>date</th>\n",
       "      <th>message_date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_sent</th>\n",
       "      <th>message_id</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sup dog</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>5.566830e+17</td>\n",
       "      <td>2018-08-22 19:04:31</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>+16142888469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[sup, dog]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hang some time sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>5.566830e+17</td>\n",
       "      <td>2018-08-22 19:04:53</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>+16142888469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[hang, some, time, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or tonight right now  lol</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>5.566830e+17</td>\n",
       "      <td>2018-08-22 19:05:08</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>+16142888469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[or, tonight, right, now,  , lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunday should work  in la tonight</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>5.566830e+17</td>\n",
       "      <td>2018-08-22 19:09:52</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>+16142888469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[sunday, should, work,  , in, la, tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hot dog  enjoy the angels</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-22</td>\n",
       "      <td>5.566830e+17</td>\n",
       "      <td>2018-08-22 19:10:15</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>+16142888469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[hot, dog,  , enjoy, the, angel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50756</th>\n",
       "      <td>just calling to catch up  call when you can  l...</td>\n",
       "      <td>36</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>6.035982e+17</td>\n",
       "      <td>2020-02-16 18:10:47</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>50781</td>\n",
       "      <td>+12564903648</td>\n",
       "      <td>33.0</td>\n",
       "      <td>[just, call, to, catch, up,  , call, when, -PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50757</th>\n",
       "      <td>i should be home around      fyi if that has a...</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>6.035982e+17</td>\n",
       "      <td>2020-02-16 18:09:52</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>50782</td>\n",
       "      <td>+12563616162</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[i, should, be, home, around,      , fyi, if, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50758</th>\n",
       "      <td>i don t expect you to wait for me and if for s...</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>6.035982e+17</td>\n",
       "      <td>2020-02-16 18:10:17</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>50783</td>\n",
       "      <td>+12563616162</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[i, don, t, expect, -PRON-, to, wait, for, -PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50759</th>\n",
       "      <td>sounds good  i went ahead and ate a little bit...</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>6.036021e+17</td>\n",
       "      <td>2020-02-16 19:14:18</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>50784</td>\n",
       "      <td>+12563616162</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[sound, good,  , i, go, ahead, and, eat, a, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50760</th>\n",
       "      <td>ugh     min  i m packing up now</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>6.036027e+17</td>\n",
       "      <td>2020-02-16 19:25:34</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>50785</td>\n",
       "      <td>+12563616162</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[ugh,     , min,  , i, m, pack, up, now]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50761 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  handle_id  \\\n",
       "0                                                sup dog          1   \n",
       "1                                 hang some time sunday           1   \n",
       "2                              or tonight right now  lol          1   \n",
       "3                      sunday should work  in la tonight          1   \n",
       "4                             hot dog  enjoy the angels           1   \n",
       "...                                                  ...        ...   \n",
       "50756  just calling to catch up  call when you can  l...         36   \n",
       "50757  i should be home around      fyi if that has a...          2   \n",
       "50758  i don t expect you to wait for me and if for s...          2   \n",
       "50759  sounds good  i went ahead and ate a little bit...          2   \n",
       "50760                    ugh     min  i m packing up now          2   \n",
       "\n",
       "             date  message_date            timestamp  month  year  is_sent  \\\n",
       "0      2018-08-22  5.566830e+17  2018-08-22 19:04:31      8  2018        1   \n",
       "1      2018-08-22  5.566830e+17  2018-08-22 19:04:53      8  2018        1   \n",
       "2      2018-08-22  5.566830e+17  2018-08-22 19:05:08      8  2018        1   \n",
       "3      2018-08-22  5.566830e+17  2018-08-22 19:09:52      8  2018        0   \n",
       "4      2018-08-22  5.566830e+17  2018-08-22 19:10:15      8  2018        1   \n",
       "...           ...           ...                  ...    ...   ...      ...   \n",
       "50756  2020-02-16  6.035982e+17  2020-02-16 18:10:47      2  2020        0   \n",
       "50757  2020-02-16  6.035982e+17  2020-02-16 18:09:52      2  2020        0   \n",
       "50758  2020-02-16  6.035982e+17  2020-02-16 18:10:17      2  2020        0   \n",
       "50759  2020-02-16  6.036021e+17  2020-02-16 19:14:18      2  2020        1   \n",
       "50760  2020-02-16  6.036027e+17  2020-02-16 19:25:34      2  2020        0   \n",
       "\n",
       "       message_id  phone_number  chat_id  \\\n",
       "0               1  +16142888469      1.0   \n",
       "1               2  +16142888469      1.0   \n",
       "2               3  +16142888469      1.0   \n",
       "3               4  +16142888469      1.0   \n",
       "4               5  +16142888469      1.0   \n",
       "...           ...           ...      ...   \n",
       "50756       50781  +12564903648     33.0   \n",
       "50757       50782  +12563616162      2.0   \n",
       "50758       50783  +12563616162      2.0   \n",
       "50759       50784  +12563616162      2.0   \n",
       "50760       50785  +12563616162      2.0   \n",
       "\n",
       "                                           tokenize_text  \n",
       "0                                             [sup, dog]  \n",
       "1                             [hang, some, time, sunday]  \n",
       "2                      [or, tonight, right, now,  , lol]  \n",
       "3             [sunday, should, work,  , in, la, tonight]  \n",
       "4                       [hot, dog,  , enjoy, the, angel]  \n",
       "...                                                  ...  \n",
       "50756  [just, call, to, catch, up,  , call, when, -PR...  \n",
       "50757  [i, should, be, home, around,      , fyi, if, ...  \n",
       "50758  [i, don, t, expect, -PRON-, to, wait, for, -PR...  \n",
       "50759  [sound, good,  , i, go, ahead, and, eat, a, li...  \n",
       "50760           [ugh,     , min,  , i, m, pack, up, now]  \n",
       "\n",
       "[50761 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Features:**  \n",
    "1. proper nouns\n",
    "2. convert emojies to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Functions for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_elongated_words(df):  # replace 'aaaaaaaahhhhh' with 'aahh'\n",
    "    counter = 0\n",
    "    for letter in ascii_lowercase:\n",
    "#         print('working on', letter)\n",
    "        for row_idx, row in enumerate(df['tokenize_text']):\n",
    "            for word_idx, word in enumerate(row):\n",
    "                original_word = word\n",
    "                while word != word.replace(letter*3, letter*2): # can strip out multiple letters?\n",
    "                    word = word.replace(letter*3, letter*2) #strip out multiple letters!\n",
    "                    df['tokenize_text'][row_idx][word_idx] = word\n",
    "#                     print(df['tokenize_text'][row_idx][word_idx])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(df):\n",
    "    for row in df['tokenize_text']:\n",
    "        for word in row:\n",
    "            if word in stopwords.words('english'):\n",
    "                row.remove(word)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stop_words(df):\n",
    "    # this takes a while\n",
    "    custom_stop_list = ['-PRON-', 'pron', ' ', '-pron-']\n",
    "#                         , 'know', 'like', 'good', 'time', 'want', 'll', 'liked'\n",
    "#                         , 'yeah', 'ya', 'yes', 'think', 'just', 'thbe'\n",
    "#                         , 'https', 'com', 'wh', 'ww', 'image', 'utm', 'fedex', 'feel'\n",
    "#                         , 'ok', 'hey', 'know', 'let', 'want', 'wan', 'na', 'need', 'a'\n",
    "#                         , 'say', 'look','love', 'oh', 'haha', 'cool', 'home', 'will', 'time']\n",
    "    \n",
    "    \n",
    "    for idx, row in enumerate(df['tokenize_text']):\n",
    "        new_row = row\n",
    "        for word in row:\n",
    "            if word in custom_stop_list:\n",
    "                new_row = [x for x in new_row if x != word]\n",
    "#                 print(idx, word, new_row)\n",
    "                if idx%10000==0:\n",
    "                    print('row', idx)\n",
    "        df['tokenize_text'][idx] = new_row\n",
    "               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_into_string(df): # in order to use CountVectorizer which intakes strings, not lists\n",
    "    for idx, row in enumerate(df['tokenize_text']):\n",
    "        df['tokenize_text'][idx] = ' '.join(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplifying words\n",
      "removing stop words\n",
      "removing custom stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\Miniconda3\\envs\\metis\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 10000\n",
      "row 20000\n",
      "row 20000\n",
      "row 30000\n",
      "detokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\Miniconda3\\envs\\metis\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# adding some prints since this one can take a minute\n",
    "# df = df.head(40)\n",
    "\n",
    "print('simplifying words')\n",
    "df = simplify_elongated_words(df)\n",
    "\n",
    "print('removing stop words') # takes the longest\n",
    "df = remove_stop_words(df)\n",
    "\n",
    "print('removing custom stop words') # maybe don't do this?\n",
    "df = custom_stop_words(df)\n",
    "\n",
    "print('detokenizing...')\n",
    "df = list_into_string(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(strip_accents='ascii', lowercase=False) #strip accents since i sometimes text in spanish\n",
    "# X = cv.fit_transform(df['tokenize_text'])\n",
    "# df2 = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop newly 'nan' or null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_rows = df[df['tokenize_text']==''].index.tolist() # find blanks and put their index into a list\n",
    "nan_rows = df[df['tokenize_text']=='nan'].index.tolist() # find 'nan's and put their index into a list\n",
    "\n",
    "df.drop(blank_rows, inplace=True)\n",
    "df.drop(nan_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('C:/users/tyler/desktop/texts_docs.pkl') # save the cleaned version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
