{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.colors as mcolors\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('c:/users/tyler/desktop/texts_docs.pkl')\n",
    "df = df['tokenize_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', min_df=3, max_df=0.8)\n",
    "tfidf = vectorizer.fit_transform(df)\n",
    "# pd.DataFrame(tfidf.toarray(), index=df, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Latent Semantic Analysis (LSA), another name for Signular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_number = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0079274 , 0.00783858, 0.00680286, 0.00571895, 0.00605046,\n",
       "       0.00580341])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(group_number)\n",
    "doc_topic = lsa.fit_transform(tfidf)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03720391952040664"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lsa.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "header = []\n",
    "for i in range(0, group_number):\n",
    "    header.append('topic'+str(i))\n",
    "\n",
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = header,\n",
    "             columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=header):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic\", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic:\",topic_names[ix])\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: topic0\n",
      "lol, yeah, like, good, yes, ok, come, love, oh, haha, think, sound, time, know, work\n",
      "\n",
      "Topic: topic1\n",
      "yeah, good, like, haha, come, ok, oh, sound, yes, think, let, love, sound good, know, time\n",
      "\n",
      "Topic: topic2\n",
      "yes, good, ok, like, come, love, sound, haha, sound good, oh, time, hey, let, know, home\n",
      "\n",
      "Topic: topic3\n",
      "ok, good, come, like, sound, love, sound good, time, know, think, haha, let, home, thank, hey\n",
      "\n",
      "Topic: topic4\n",
      "ok, cool, ok cool, yeah, yes, haha ok, ok let, ok ok, ok thank, lol ok, ok pack, cool cool, ok head, oh ok, pack\n",
      "\n",
      "Topic: topic5\n",
      "come, home, come home, hey, like, work, wanna, want, wanna come, soon, tonight, home soon, want come, sorry, nice\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenize_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come to un plaza</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come home</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come late</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come home</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come back</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come home</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come back</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come find here</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come home</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come home</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just go be sporadic come</th>\n",
       "      <td>0.051</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t hear         i m come get</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             topic0  topic1  topic2  topic3  topic4  topic5\n",
       "tokenize_text                                                              \n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "may come                      0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come to un plaza              0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come home                     0.050   0.044   0.064   0.205  -0.056   0.608\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come late                     0.037   0.034   0.048   0.160  -0.048   0.524\n",
       "come home                     0.050   0.044   0.064   0.205  -0.056   0.608\n",
       "come back                     0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come home                     0.050   0.044   0.064   0.205  -0.056   0.608\n",
       "come back                     0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come find here                0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come home                     0.050   0.044   0.064   0.205  -0.056   0.608\n",
       "come home                     0.050   0.044   0.064   0.205  -0.056   0.608\n",
       "i come                        0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "just go be sporadic come      0.051   0.045   0.062   0.200  -0.061   0.596\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "y come                        0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "t hear         i m come get   0.036   0.036   0.049   0.168  -0.053   0.529\n",
       "    come                      0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "i come                        0.052   0.050   0.071   0.249  -0.075   0.863\n",
       "come                          0.052   0.050   0.071   0.249  -0.075   0.863"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(3), index = df, columns = header)\n",
    "Vt[Vt[:]['topic5'] > 0.5][140:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.stop_words_ # see the words removed with min_df and max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this in a moment\n",
    "data = Vt.idxmax(axis=1)\n",
    "data = data.reset_index()\n",
    "data['category'] = data[0]\n",
    "data.drop(labels=0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>31796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>7479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic5</th>\n",
       "      <td>2919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tokenize_text\n",
       "category               \n",
       "topic3            31796\n",
       "topic0             7479\n",
       "topic5             2919\n",
       "topic1             2261\n",
       "topic2             1824\n",
       "topic4             1107"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bars = data.groupby('category').count().sort_values(by='tokenize_text', ascending=False)\n",
    "bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('c:/users/tyler/desktop/LSA_topics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common = sorted(list(zip(vectorizer.get_feature_names(), vectorizer.idf_)), key = lambda t: t[1])\n",
    "# for elem in most_common[0]:\n",
    "#     print(most_common, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
