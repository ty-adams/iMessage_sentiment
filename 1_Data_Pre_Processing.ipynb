{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from string import ascii_lowercase\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the file previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/users/tyler/desktop/texts_lammetized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Features:**  \n",
    "1. proper nouns\n",
    "2. convert emojies to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Functions for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_elongated_words(df):  # replace 'aaaaaaaahhhhh' with 'aahh'\n",
    "    counter = 0\n",
    "    for letter in ascii_lowercase:\n",
    "#         print('working on', letter)\n",
    "        for row_idx, row in enumerate(df['tokenize_text']):\n",
    "            for word_idx, word in enumerate(row):\n",
    "                original_word = word\n",
    "                while word != word.replace(letter*3, letter*2): # can strip out multiple letters?\n",
    "                    word = word.replace(letter*3, letter*2) #strip out multiple letters!\n",
    "                    df['tokenize_text'][row_idx][word_idx] = word\n",
    "#                     print(df['tokenize_text'][row_idx][word_idx])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(df):\n",
    "    for row in df['tokenize_text']:\n",
    "        for word in row:\n",
    "            if word in stopwords.words('english'):\n",
    "                row.remove(word)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stop_words(df):\n",
    "    # this takes a while\n",
    "    custom_stop_list = ['-PRON-', 'pron', ' ', '-pron-']\n",
    "#                         , 'know', 'like', 'good', 'time', 'want', 'll', 'liked'\n",
    "#                         , 'yeah', 'ya', 'yes', 'think', 'just', 'thbe'\n",
    "#                         , 'https', 'com', 'wh', 'ww', 'image', 'utm', 'fedex', 'feel'\n",
    "#                         , 'ok', 'hey', 'know', 'let', 'want', 'wan', 'na', 'need', 'a'\n",
    "#                         , 'say', 'look','love', 'oh', 'haha', 'cool', 'home', 'will', 'time']\n",
    "    \n",
    "    \n",
    "    for idx, row in enumerate(df['tokenize_text']):\n",
    "        new_row = row\n",
    "        for word in row:\n",
    "            if word in custom_stop_list:\n",
    "                new_row = [x for x in new_row if x != word]\n",
    "#                 print(idx, word, new_row)\n",
    "                if idx%10000==0:\n",
    "                    print('row', idx)\n",
    "        df['tokenize_text'][idx] = new_row\n",
    "               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_into_string(df): # in order to use CountVectorizer which intakes strings, not lists\n",
    "    for idx, row in enumerate(df['tokenize_text']):\n",
    "        df['tokenize_text'][idx] = ' '.join(row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adding some prints since this one can take a minute\n",
    "# df = df.head(40)\n",
    "\n",
    "print('simplifying words')\n",
    "df = simplify_elongated_words(df)\n",
    "\n",
    "print('removing stop words') # takes the longest\n",
    "df = remove_stop_words(df)\n",
    "\n",
    "print('removing custom stop words') # maybe don't do this?\n",
    "df = custom_stop_words(df)\n",
    "\n",
    "print('detokenizing...')\n",
    "df = list_into_string(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(strip_accents='ascii', lowercase=False) #strip accents since i sometimes text in spanish\n",
    "# X = cv.fit_transform(df['tokenize_text'])\n",
    "# df2 = pd.DataFrame(X.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop newly 'nan' or null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_rows = df[df['tokenize_text']==''].index.tolist() # find blanks and put their index into a list\n",
    "nan_rows = df[df['tokenize_text']=='nan'].index.tolist() # find 'nan's and put their index into a list\n",
    "\n",
    "df.drop(blank_rows, inplace=True)\n",
    "df.drop(nan_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('C:/users/tyler/desktop/texts_docs.pkl') # save the cleaned version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
